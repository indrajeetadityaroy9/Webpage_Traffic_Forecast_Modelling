{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:23.980929Z",
     "start_time": "2024-11-18T15:01:23.979363Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Overview"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb893b07f04adbc5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                Page  2015-07-01  2015-07-02  \\\n0            2NE1_zh.wikipedia.org_all-access_spider        18.0        11.0   \n1             2PM_zh.wikipedia.org_all-access_spider        11.0        14.0   \n2              3C_zh.wikipedia.org_all-access_spider         1.0         0.0   \n3         4minute_zh.wikipedia.org_all-access_spider        35.0        13.0   \n4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...         NaN         NaN   \n\n   2015-07-03  2015-07-04  2015-07-05  2015-07-06  2015-07-07  2015-07-08  \\\n0         5.0        13.0        14.0         9.0         9.0        22.0   \n1        15.0        18.0        11.0        13.0        22.0        11.0   \n2         1.0         1.0         0.0         4.0         0.0         3.0   \n3        10.0        94.0         4.0        26.0        14.0         9.0   \n4         NaN         NaN         NaN         NaN         NaN         NaN   \n\n   2015-07-09  ...  2016-12-22  2016-12-23  2016-12-24  2016-12-25  \\\n0        26.0  ...        32.0        63.0        15.0        26.0   \n1        10.0  ...        17.0        42.0        28.0        15.0   \n2         4.0  ...         3.0         1.0         1.0         7.0   \n3        11.0  ...        32.0        10.0        26.0        27.0   \n4         NaN  ...        48.0         9.0        25.0        13.0   \n\n   2016-12-26  2016-12-27  2016-12-28  2016-12-29  2016-12-30  2016-12-31  \n0        14.0        20.0        22.0        19.0        18.0        20.0  \n1         9.0        30.0        52.0        45.0        26.0        20.0  \n2         4.0         4.0         6.0         3.0         4.0        17.0  \n3        16.0        11.0        17.0        19.0        10.0        11.0  \n4         3.0        11.0        27.0        13.0        36.0        10.0  \n\n[5 rows x 551 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page</th>\n      <th>2015-07-01</th>\n      <th>2015-07-02</th>\n      <th>2015-07-03</th>\n      <th>2015-07-04</th>\n      <th>2015-07-05</th>\n      <th>2015-07-06</th>\n      <th>2015-07-07</th>\n      <th>2015-07-08</th>\n      <th>2015-07-09</th>\n      <th>...</th>\n      <th>2016-12-22</th>\n      <th>2016-12-23</th>\n      <th>2016-12-24</th>\n      <th>2016-12-25</th>\n      <th>2016-12-26</th>\n      <th>2016-12-27</th>\n      <th>2016-12-28</th>\n      <th>2016-12-29</th>\n      <th>2016-12-30</th>\n      <th>2016-12-31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n      <td>18.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>22.0</td>\n      <td>26.0</td>\n      <td>...</td>\n      <td>32.0</td>\n      <td>63.0</td>\n      <td>15.0</td>\n      <td>26.0</td>\n      <td>14.0</td>\n      <td>20.0</td>\n      <td>22.0</td>\n      <td>19.0</td>\n      <td>18.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n      <td>11.0</td>\n      <td>14.0</td>\n      <td>15.0</td>\n      <td>18.0</td>\n      <td>11.0</td>\n      <td>13.0</td>\n      <td>22.0</td>\n      <td>11.0</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>17.0</td>\n      <td>42.0</td>\n      <td>28.0</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>30.0</td>\n      <td>52.0</td>\n      <td>45.0</td>\n      <td>26.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3C_zh.wikipedia.org_all-access_spider</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n      <td>35.0</td>\n      <td>13.0</td>\n      <td>10.0</td>\n      <td>94.0</td>\n      <td>4.0</td>\n      <td>26.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n      <td>11.0</td>\n      <td>...</td>\n      <td>32.0</td>\n      <td>10.0</td>\n      <td>26.0</td>\n      <td>27.0</td>\n      <td>16.0</td>\n      <td>11.0</td>\n      <td>17.0</td>\n      <td>19.0</td>\n      <td>10.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>48.0</td>\n      <td>9.0</td>\n      <td>25.0</td>\n      <td>13.0</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>27.0</td>\n      <td>13.0</td>\n      <td>36.0</td>\n      <td>10.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 551 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_1.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:28.258677Z",
     "start_time": "2024-11-18T15:01:25.564440Z"
    }
   },
   "id": "b9f72713620b5e77"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145063 entries, 0 to 145062\n",
      "Columns: 551 entries, Page to 2016-12-31\n",
      "dtypes: float64(550), object(1)\n",
      "memory usage: 609.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:29.955113Z",
     "start_time": "2024-11-18T15:01:29.952730Z"
    }
   },
   "id": "138ddea0a8e4a2cc"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Page           object\n2015-07-01    float64\n2015-07-02    float64\n2015-07-03    float64\n2015-07-04    float64\n               ...   \n2016-12-27    float64\n2016-12-28    float64\n2016-12-29    float64\n2016-12-30    float64\n2016-12-31    float64\nLength: 551, dtype: object"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:33.050207Z",
     "start_time": "2024-11-18T15:01:33.039905Z"
    }
   },
   "id": "6a7a42cb86cc36af"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ranges from 2015-07-01 to 2016-12-31\n"
     ]
    }
   ],
   "source": [
    "data_start_date = df.columns[1]\n",
    "data_end_date = df.columns[-1]\n",
    "print('Data ranges from %s to %s' % (data_start_date, data_end_date))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:35.171163Z",
     "start_time": "2024-11-18T15:01:35.160111Z"
    }
   },
   "id": "1465ead0313b393f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                Page  2015-07-01  2015-07-02  \\\n0            2NE1_zh.wikipedia.org_all-access_spider        18.0        11.0   \n1             2PM_zh.wikipedia.org_all-access_spider        11.0        14.0   \n2              3C_zh.wikipedia.org_all-access_spider         1.0         0.0   \n3         4minute_zh.wikipedia.org_all-access_spider        35.0        13.0   \n4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...         NaN         NaN   \n\n   2015-07-03  2015-07-04  2015-07-05  2015-07-06  2015-07-07  2015-07-08  \\\n0         5.0        13.0        14.0         9.0         9.0        22.0   \n1        15.0        18.0        11.0        13.0        22.0        11.0   \n2         1.0         1.0         0.0         4.0         0.0         3.0   \n3        10.0        94.0         4.0        26.0        14.0         9.0   \n4         NaN         NaN         NaN         NaN         NaN         NaN   \n\n   2015-07-09  ...  2017-09-01  2017-09-02  2017-09-03  2017-09-04  \\\n0        26.0  ...        19.0        33.0        33.0        18.0   \n1        10.0  ...        32.0        30.0        11.0        19.0   \n2         4.0  ...         6.0         6.0         7.0         2.0   \n3        11.0  ...         7.0        19.0        19.0         9.0   \n4         NaN  ...        16.0        16.0        19.0         9.0   \n\n   2017-09-05  2017-09-06  2017-09-07  2017-09-08  2017-09-09  2017-09-10  \n0        16.0        27.0        29.0        23.0        54.0        38.0  \n1        54.0        25.0        26.0        23.0        13.0        81.0  \n2         4.0         7.0         3.0         4.0         7.0         6.0  \n3         6.0        16.0        19.0        30.0        38.0         4.0  \n4        20.0        23.0        28.0        14.0         8.0         7.0  \n\n[5 rows x 804 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page</th>\n      <th>2015-07-01</th>\n      <th>2015-07-02</th>\n      <th>2015-07-03</th>\n      <th>2015-07-04</th>\n      <th>2015-07-05</th>\n      <th>2015-07-06</th>\n      <th>2015-07-07</th>\n      <th>2015-07-08</th>\n      <th>2015-07-09</th>\n      <th>...</th>\n      <th>2017-09-01</th>\n      <th>2017-09-02</th>\n      <th>2017-09-03</th>\n      <th>2017-09-04</th>\n      <th>2017-09-05</th>\n      <th>2017-09-06</th>\n      <th>2017-09-07</th>\n      <th>2017-09-08</th>\n      <th>2017-09-09</th>\n      <th>2017-09-10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n      <td>18.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>22.0</td>\n      <td>26.0</td>\n      <td>...</td>\n      <td>19.0</td>\n      <td>33.0</td>\n      <td>33.0</td>\n      <td>18.0</td>\n      <td>16.0</td>\n      <td>27.0</td>\n      <td>29.0</td>\n      <td>23.0</td>\n      <td>54.0</td>\n      <td>38.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n      <td>11.0</td>\n      <td>14.0</td>\n      <td>15.0</td>\n      <td>18.0</td>\n      <td>11.0</td>\n      <td>13.0</td>\n      <td>22.0</td>\n      <td>11.0</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>32.0</td>\n      <td>30.0</td>\n      <td>11.0</td>\n      <td>19.0</td>\n      <td>54.0</td>\n      <td>25.0</td>\n      <td>26.0</td>\n      <td>23.0</td>\n      <td>13.0</td>\n      <td>81.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3C_zh.wikipedia.org_all-access_spider</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>6.0</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>7.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n      <td>35.0</td>\n      <td>13.0</td>\n      <td>10.0</td>\n      <td>94.0</td>\n      <td>4.0</td>\n      <td>26.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n      <td>11.0</td>\n      <td>...</td>\n      <td>7.0</td>\n      <td>19.0</td>\n      <td>19.0</td>\n      <td>9.0</td>\n      <td>6.0</td>\n      <td>16.0</td>\n      <td>19.0</td>\n      <td>30.0</td>\n      <td>38.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>16.0</td>\n      <td>16.0</td>\n      <td>19.0</td>\n      <td>9.0</td>\n      <td>20.0</td>\n      <td>23.0</td>\n      <td>28.0</td>\n      <td>14.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 804 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_2.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:41.295686Z",
     "start_time": "2024-11-18T15:01:37.286112Z"
    }
   },
   "id": "21695daccddb9135"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ranges from 2015-07-01 to 2017-09-10\n"
     ]
    }
   ],
   "source": [
    "data_start_date = df.columns[1]\n",
    "data_end_date = df.columns[-1]\n",
    "print('Data ranges from %s to %s' % (data_start_date, data_end_date))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:44.621882Z",
     "start_time": "2024-11-18T15:01:44.618298Z"
    }
   },
   "id": "7679716ed73104e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check and confirmation of the date overlap between the two datasets (Train1.csv and Train2.csv)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de1a0de8b1808574"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates overlap from 2015-07-01 00:00:00 to 2016-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train1 = pd.read_csv('train_1.csv')\n",
    "train2 = pd.read_csv('train_2.csv')\n",
    "\n",
    "date_columns_train1 = train1.columns[1:]\n",
    "date_columns_train2 = train2.columns[1:]\n",
    "\n",
    "train1_dates = pd.to_datetime(date_columns_train1)\n",
    "train2_dates = pd.to_datetime(date_columns_train2)\n",
    "\n",
    "train1_min_date = train1_dates.min()\n",
    "train1_max_date = train1_dates.max()\n",
    "train2_min_date = train2_dates.min()\n",
    "train2_max_date = train2_dates.max()\n",
    "\n",
    "overlap_start = max(train1_min_date, train2_min_date)\n",
    "overlap_end = min(train1_max_date, train2_max_date)\n",
    "\n",
    "if overlap_start <= overlap_end:\n",
    "    print(f\"Dates overlap from {overlap_start} to {overlap_end}\")\n",
    "else:\n",
    "    print(\"No overlap between the date ranges of Train1 and Train2.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:01:55.270310Z",
     "start_time": "2024-11-18T15:01:48.391137Z"
    }
   },
   "id": "e5d1b8d6df9319af"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "page_names_train1 = train1.iloc[:, 0]\n",
    "page_names_train2 = train2.iloc[:, 0]\n",
    "\n",
    "# Filter the date columns based on the overlapping range\n",
    "date_columns_overlap = train1.columns[1:]\n",
    "overlap_columns = ['Page'] + list(date_columns_overlap)\n",
    "\n",
    "train1_overlap = train1[overlap_columns]\n",
    "train2_overlap = train2[overlap_columns]\n",
    "\n",
    "# Merge the two datasets based on the page name\n",
    "merged_data = pd.merge(train1_overlap, train2_overlap, on='Page', suffixes=('_train1', '_train2'))\n",
    "merged_data.columns = merged_data.columns.str.replace('_train1', '').str.replace('_train2', '')\n",
    "final_data = pd.DataFrame()\n",
    "final_data['Page'] = merged_data['Page']\n",
    "\n",
    "columns_to_merge = []\n",
    "\n",
    "for col in date_columns_overlap:\n",
    "    merged_column = merged_data[col].mean(axis=1)\n",
    "    columns_to_merge.append(pd.DataFrame({col: merged_column}))\n",
    "\n",
    "final_data = pd.concat([final_data] + columns_to_merge, axis=1)\n",
    "final_data.to_csv('Merged_Wikipedia_Traffic.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:02:22.882898Z",
     "start_time": "2024-11-18T15:01:56.716953Z"
    }
   },
   "id": "f04ee6d2fbf1bb6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                Page  2015-07-01  2015-07-02  \\\n0            2NE1_zh.wikipedia.org_all-access_spider        18.0        11.0   \n1             2PM_zh.wikipedia.org_all-access_spider        11.0        14.0   \n2              3C_zh.wikipedia.org_all-access_spider         1.0         0.0   \n3         4minute_zh.wikipedia.org_all-access_spider        35.0        13.0   \n4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...         NaN         NaN   \n\n   2015-07-03  2015-07-04  2015-07-05  2015-07-06  2015-07-07  2015-07-08  \\\n0         5.0        13.0        14.0         9.0         9.0        22.0   \n1        15.0        18.0        11.0        13.0        22.0        11.0   \n2         1.0         1.0         0.0         4.0         0.0         3.0   \n3        10.0        94.0         4.0        26.0        14.0         9.0   \n4         NaN         NaN         NaN         NaN         NaN         NaN   \n\n   2015-07-09  ...  2016-12-22  2016-12-23  2016-12-24  2016-12-25  \\\n0        26.0  ...        32.0        63.0        15.0        26.0   \n1        10.0  ...        17.0        42.0        28.0        15.0   \n2         4.0  ...         3.0         1.0         1.0         7.0   \n3        11.0  ...        32.0        10.0        26.0        27.0   \n4         NaN  ...        48.0         9.0        25.0        13.0   \n\n   2016-12-26  2016-12-27  2016-12-28  2016-12-29  2016-12-30  2016-12-31  \n0        14.0        20.0        22.0        19.0        18.0        20.0  \n1         9.0        30.0        52.0        45.0        26.0        20.0  \n2         4.0         4.0         6.0         3.0         4.0        17.0  \n3        16.0        11.0        17.0        19.0        10.0        11.0  \n4         3.0        11.0        27.0        13.0        36.0        10.0  \n\n[5 rows x 551 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Page</th>\n      <th>2015-07-01</th>\n      <th>2015-07-02</th>\n      <th>2015-07-03</th>\n      <th>2015-07-04</th>\n      <th>2015-07-05</th>\n      <th>2015-07-06</th>\n      <th>2015-07-07</th>\n      <th>2015-07-08</th>\n      <th>2015-07-09</th>\n      <th>...</th>\n      <th>2016-12-22</th>\n      <th>2016-12-23</th>\n      <th>2016-12-24</th>\n      <th>2016-12-25</th>\n      <th>2016-12-26</th>\n      <th>2016-12-27</th>\n      <th>2016-12-28</th>\n      <th>2016-12-29</th>\n      <th>2016-12-30</th>\n      <th>2016-12-31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n      <td>18.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>22.0</td>\n      <td>26.0</td>\n      <td>...</td>\n      <td>32.0</td>\n      <td>63.0</td>\n      <td>15.0</td>\n      <td>26.0</td>\n      <td>14.0</td>\n      <td>20.0</td>\n      <td>22.0</td>\n      <td>19.0</td>\n      <td>18.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n      <td>11.0</td>\n      <td>14.0</td>\n      <td>15.0</td>\n      <td>18.0</td>\n      <td>11.0</td>\n      <td>13.0</td>\n      <td>22.0</td>\n      <td>11.0</td>\n      <td>10.0</td>\n      <td>...</td>\n      <td>17.0</td>\n      <td>42.0</td>\n      <td>28.0</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>30.0</td>\n      <td>52.0</td>\n      <td>45.0</td>\n      <td>26.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3C_zh.wikipedia.org_all-access_spider</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n      <td>35.0</td>\n      <td>13.0</td>\n      <td>10.0</td>\n      <td>94.0</td>\n      <td>4.0</td>\n      <td>26.0</td>\n      <td>14.0</td>\n      <td>9.0</td>\n      <td>11.0</td>\n      <td>...</td>\n      <td>32.0</td>\n      <td>10.0</td>\n      <td>26.0</td>\n      <td>27.0</td>\n      <td>16.0</td>\n      <td>11.0</td>\n      <td>17.0</td>\n      <td>19.0</td>\n      <td>10.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>48.0</td>\n      <td>9.0</td>\n      <td>25.0</td>\n      <td>13.0</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>27.0</td>\n      <td>13.0</td>\n      <td>36.0</td>\n      <td>10.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 551 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Merged_Wikipedia_Traffic.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:02:48.492546Z",
     "start_time": "2024-11-18T15:02:45.183995Z"
    }
   },
   "id": "17fe3f2452b4b10d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145063 entries, 0 to 145062\n",
      "Columns: 551 entries, Page to 2016-12-31\n",
      "dtypes: float64(550), object(1)\n",
      "memory usage: 609.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:02:58.623017Z",
     "start_time": "2024-11-18T15:02:58.620874Z"
    }
   },
   "id": "175b147e9b65e9c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31b68d1db4d7fac8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identify Missing Values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3f28c9222696c0f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "2015-07-01    20740\n2015-07-02    20816\n2015-07-03    20544\n2015-07-04    20654\n2015-07-05    20659\n              ...  \n2016-12-27     3701\n2016-12-28     3822\n2016-12-29     3826\n2016-12-30     3635\n2016-12-31     3465\nLength: 550, dtype: int64"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0]\n",
    "missing_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:03:03.695451Z",
     "start_time": "2024-11-18T15:03:03.527579Z"
    }
   },
   "id": "9c0bddcdfd6d8203"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "2015-07-02    14.35%\n2015-07-01    14.30%\n2015-07-07    14.24%\n2015-07-05    14.24%\n2015-07-04    14.24%\n               ...  \n2016-12-12     2.44%\n2016-12-31     2.39%\n2016-12-20     2.25%\n2016-12-21     2.23%\n2016-12-24     2.20%\nLength: 550, dtype: object"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data_proportion = df.isnull().mean() * 100\n",
    "missing_data_proportion = missing_data_proportion[missing_data_proportion > 0]\n",
    "missing_data_proportion = missing_data_proportion.sort_values(ascending=False)\n",
    "missing_data_proportion_formatted = missing_data_proportion.apply(lambda x: f\"{x:.2f}%\")\n",
    "missing_data_proportion_formatted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:03:15.614445Z",
     "start_time": "2024-11-18T15:03:15.543787Z"
    }
   },
   "id": "3e95ecfc1ec6ec82"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage of missing data per page: 7.76%\n",
      "Average percentage of missing data per date: 7.75%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of missing values per column (date)\n",
    "missing_values_per_date = df.isna().sum()\n",
    "# Calculate the total number of missing values per row (page)\n",
    "missing_values_per_page = df.isna().sum(axis=1)\n",
    "# Percentage of missing values per date\n",
    "percentage_missing_per_date = (missing_values_per_date / len(df)) * 100\n",
    "# Percentage of missing values per page\n",
    "percentage_missing_per_page = (missing_values_per_page / (df.shape[1] - 1)) * 100\n",
    "print(\"Average percentage of missing data per page: {:.2f}%\".format(percentage_missing_per_page.mean()))\n",
    "print(\"Average percentage of missing data per date: {:.2f}%\".format(percentage_missing_per_date.mean()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:03:23.056833Z",
     "start_time": "2024-11-18T15:03:22.853856Z"
    }
   },
   "id": "7ae84a8fedba20b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handle Missing Values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81d528e3efc7b45e"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Data Types:\n",
      "Page                    object\n",
      "2015-07-01 00:00:00    float64\n",
      "2015-07-02 00:00:00    float64\n",
      "2015-07-03 00:00:00    float64\n",
      "2015-07-04 00:00:00    float64\n",
      "                        ...   \n",
      "2016-12-28 00:00:00    float64\n",
      "2016-12-29 00:00:00    float64\n",
      "2016-12-30 00:00:00    float64\n",
      "2016-12-31 00:00:00    float64\n",
      "Average_Traffic        float64\n",
      "Length: 552, dtype: object\n"
     ]
    }
   ],
   "source": [
    "date_cols = df.columns.drop('Page')\n",
    "df.rename(columns=lambda x: pd.to_datetime(x, errors='ignore') if x != 'Page' else x, inplace=True)\n",
    "print(\"\\nColumn Data Types:\")\n",
    "print(df.dtypes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:16:20.885229Z",
     "start_time": "2024-11-18T15:16:20.869315Z"
    }
   },
   "id": "8b9e75485b7cdb9d"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traffic Threshold (Median of Average Traffic): 274.91\n",
      "\n",
      "Number of High-Traffic Pages: 72205\n",
      "Number of Low-Traffic Pages: 72206\n"
     ]
    }
   ],
   "source": [
    "# Segment Pages Based on Traffic Patterns\n",
    "# Calculate the average traffic per page\n",
    "df_numeric = df.drop(columns=['Page'])\n",
    "df['Average_Traffic'] = df_numeric.mean(axis=1)\n",
    "# Determine a traffic threshold\n",
    "traffic_threshold = df['Average_Traffic'].median()\n",
    "print(\"\\nTraffic Threshold (Median of Average Traffic): {:.2f}\".format(traffic_threshold))\n",
    "# Segment pages into high and low traffic based on the threshold\n",
    "high_traffic_df = df[df['Average_Traffic'] > traffic_threshold].copy()\n",
    "low_traffic_df = df[df['Average_Traffic'] <= traffic_threshold].copy()\n",
    "print(\"\\nNumber of High-Traffic Pages:\", len(high_traffic_df))\n",
    "print(\"Number of Low-Traffic Pages:\", len(low_traffic_df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:16:25.114814Z",
     "start_time": "2024-11-18T15:16:23.589505Z"
    }
   },
   "id": "99a76b4a78432e28"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def interpolate_missing_values(df):\n",
    "    df = df.set_index('Page')\n",
    "    df_dates = df.drop(columns=['Average_Traffic'])\n",
    "    df_transposed = df_dates.transpose()\n",
    "    df_transposed.index = pd.to_datetime(df_transposed.index)\n",
    "    df_interpolated = df_transposed.interpolate(method='time', limit_direction='both', axis=0)\n",
    "    df_interpolated = df_interpolated.transpose().reset_index()\n",
    "    return df_interpolated\n",
    "\n",
    "# Function to fill missing values with zeros for low-traffic pages\n",
    "def fill_missing_with_zero(df):\n",
    "    df = df.drop(columns=['Average_Traffic'])\n",
    "    df_filled = df.fillna(0)\n",
    "    return df_filled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:16:58.914605Z",
     "start_time": "2024-11-18T15:16:58.908807Z"
    }
   },
   "id": "a08845701e6bbfc9"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total missing values remaining after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# Apply interpolation to high-traffic pages\n",
    "high_traffic_cleaned = interpolate_missing_values(high_traffic_df)\n",
    "# Apply zero-filling to low-traffic pages\n",
    "low_traffic_cleaned = fill_missing_with_zero(low_traffic_df)\n",
    "# Combine the cleaned datasets\n",
    "df_cleaned = pd.concat([high_traffic_cleaned, low_traffic_cleaned], ignore_index=True)\n",
    "\n",
    "total_missing_values = df_cleaned.isna().sum().sum()\n",
    "print(\"\\nTotal missing values remaining after cleaning:\", total_missing_values)\n",
    "\n",
    "if 'Average_Traffic' in df_cleaned.columns:\n",
    "    df_cleaned = df_cleaned.drop(columns=['Average_Traffic'])\n",
    "\n",
    "df_cleaned.to_csv('Cleaned_Wikipedia_Traffic.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:18:33.401115Z",
     "start_time": "2024-11-18T15:18:13.313851Z"
    }
   },
   "id": "82597702038e3069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Distribution and Univariate Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e210c390283b5d3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Smoothing and log1p transformation are applied to web traffic data to normalize the data, reduce noise, and allow for clearer visualizations of the traffic.\n",
    "\n",
    "Smoothing, using a rolling window (30 days), reduces short-term fluctuations, revealing long-term trends by averaging values over a set period. The log1p transformation compresses large traffic values and handles zero counts effectively, making differences in traffic across pages more comparable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4839fb79b7ca16f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_wikipedia_traffic(merged_data, pages_to_compare=9, window_size=30, show_by_month=True):\n",
    "    selected_pages = np.random.choice(merged_data['Page'].unique(), size=pages_to_compare, replace=False)\n",
    "\n",
    "    for i in range(0, len(selected_pages), 3):\n",
    "        subset_pages = selected_pages[i:i + 3]\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        for page in subset_pages:\n",
    "            page_data = merged_data[merged_data['Page'] == page].set_index('Page').T\n",
    "            page_data.index = pd.to_datetime(page_data.index, errors='coerce')\n",
    "            page_data = page_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Calculate the average page views (non-log-transformed)\n",
    "            average_page_views = page_data.mean().values[0]\n",
    "            # Apply log1p transformation\n",
    "            page_data_log = np.log1p(page_data)\n",
    "            # Smooth traffic data using a rolling window\n",
    "            page_data_smoothed = page_data_log.rolling(window=window_size, min_periods=1).mean()\n",
    "            name_project = '_'.join(page.rsplit('_', 2)[:-2])\n",
    "            plt.plot(page_data.index, page_data_smoothed, label=f'{name_project} (Avg: {average_page_views:.0f} views)')\n",
    "\n",
    "        if show_by_month:\n",
    "            plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Avg Daily Page Views (log1p)', fontsize=12)\n",
    "        plt.gcf().autofmt_xdate(rotation=45)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24655e3454a9a0b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_data = pd.read_csv('Merged_Wikipedia_Traffic.csv')\n",
    "plot_wikipedia_traffic(merged_data, pages_to_compare=9, window_size=30, show_by_month=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b0fe55566ff93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merged_data = pd.read_csv('Merged_Wikipedia_Traffic.csv')# Extract components from the Page\n",
    "merged_data['Topic'] = merged_data['Page'].apply(lambda x: '_'.join(x.split('_')[:-3]))  # Extract topic\n",
    "merged_data['Language'] = merged_data['Page'].apply(lambda x: x.split('_')[-3].split('.')[0])  # Extract language\n",
    "merged_data['Access_Type'] = merged_data['Page'].apply(lambda x: x.split('_')[-2])  # Extract access type\n",
    "merged_data['Agent'] = merged_data['Page'].apply(lambda x: x.split('_')[-1])  # Extract data agent\n",
    "\n",
    "# list of valid languages (ISO codes for common Wikipedia languages)\n",
    "valid_languages = ['en', 'zh', 'fr', 'de', 'es', 'ja', 'ru', 'it', 'pt', 'nl', 'ar', 'sv', 'pl', 'vi', 'ko', 'he', 'cs']\n",
    "\n",
    "merged_data_filtered = merged_data[merged_data['Language'].isin(valid_languages)]\n",
    "language_distribution = merged_data_filtered['Language'].value_counts()\n",
    "plt.figure(figsize=(8, 4))\n",
    "language_distribution.plot(kind='bar', color='blue')\n",
    "plt.title('Distribution of Wikipedia Pages by Language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Number of Pages')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a045be349ad7c3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the merged dataset\n",
    "merged_data = pd.read_csv('Merged_Wikipedia_Traffic.csv')\n",
    "\n",
    "# Extract 'Topic', 'Language', 'Access_Type', 'Agent' from the 'Page' column\n",
    "merged_data['Topic'] = merged_data['Page'].apply(lambda x: '_'.join(x.split('_')[:-3]))  # Extract topic\n",
    "merged_data['Language'] = merged_data['Page'].apply(lambda x: x.split('_')[-3].split('.')[0])  # Extract language\n",
    "merged_data['Access_Type'] = merged_data['Page'].apply(lambda x: x.split('_')[-2])  # Extract access type\n",
    "merged_data['Agent'] = merged_data['Page'].apply(lambda x: x.split('_')[-1])  # Extract data agent\n",
    "\n",
    "# List of valid languages (ISO codes for common Wikipedia languages)\n",
    "valid_languages = ['en', 'zh', 'fr', 'de', 'es', 'ja', 'ru', 'it', 'pt', 'nl', 'ar', 'sv', 'pl', 'vi', 'ko', 'he', 'cs']\n",
    "\n",
    "# Filter to keep only rows with valid languages\n",
    "merged_data_filtered = merged_data[merged_data['Language'].isin(valid_languages)]\n",
    "\n",
    "# Extract the date columns from the dataframe\n",
    "date_columns = merged_data.columns[1:-4]  # Exclude non-date columns like 'Page', 'Topic', 'Language', etc.\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "date_columns = pd.to_datetime(date_columns, format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Update the merged_data's columns to keep the correct column names\n",
    "merged_data_filtered.columns = ['Page'] + list(date_columns) + ['Topic', 'Language', 'Access_Type', 'Agent']\n",
    "\n",
    "# Filter out any rows where dates are invalid\n",
    "merged_data_filtered = merged_data_filtered.dropna(axis=1, how='all')\n",
    "\n",
    "# List of languages to compare\n",
    "languages = ['en', 'ja', 'de', 'fr', 'zh', 'ru', 'es']\n",
    "\n",
    "# Prepare language-based traffic sums\n",
    "lang_sets = {}\n",
    "for lang in languages:\n",
    "    lang_data = merged_data_filtered[merged_data_filtered['Language'] == lang].iloc[:, 1:len(date_columns) + 1]\n",
    "    if not lang_data.empty:\n",
    "        lang_sets[lang] = lang_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Create a dictionary to store average traffic per language\n",
    "sums = {}\n",
    "for key in lang_sets:\n",
    "    num_pages = lang_sets[key].shape[0]\n",
    "    if num_pages > 0:\n",
    "        sums[key] = lang_sets[key].sum(axis=0) / num_pages\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for key, avg_traffic in sums.items():\n",
    "    if not avg_traffic.empty:\n",
    "        plt.plot(date_columns, avg_traffic.values, label=f'{key}', linewidth=2)\n",
    "\n",
    "plt.title('Average Wikipedia Traffic per Language Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Avg Daily Page Views', fontsize=12)\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=2))  \n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  \n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend(title='Language', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dfcd941aeb42ed1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter the dataset for rows where Language is 'www'\n",
    "filtered_data = merged_data[merged_data['Language'] == 'www']\n",
    "\n",
    "# Output the topics associated with 'www'\n",
    "topics_www = filtered_data[['Page', 'Topic', 'Language']]\n",
    "print(f\"Total number of topics with Language 'www': {len(topics_www)}\")\n",
    "print(topics_www.head())\n",
    "\n",
    "# Extract the prefix of the topics\n",
    "filtered_data['Subcategory'] = filtered_data['Topic'].apply(lambda x: x.split(':')[0].split('_')[0])\n",
    "\n",
    "# Count the occurrences of each subcategory\n",
    "subcategory_counts = filtered_data['Subcategory'].value_counts()\n",
    "print(\"Top 10 subcategories for 'www':\")\n",
    "print(subcategory_counts.head(10))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "subcategory_counts.head(10).plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Subcategories for \"www\" Language Topics')\n",
    "plt.xlabel('Subcategory')\n",
    "plt.ylabel('Count of Topics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff033c7ecf912dc8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The topics associated with the 'www' language are primarily related to the technical implementation, documentation, and maintenance aspects of the Wikimedia platform. These include help manuals, API references, extensions, and project documentation rather than typical encyclopedia-style content you’d find on standard Wikipedia pages in other languages (e.g., 'en', 'fr', 'zh', etc.).\n",
    "\n",
    "The regular Wikipedia pages in different languages (like 'en', 'fr', or 'zh') focus on various subjects and articles, ranging from history to science, pop culture, and more, catering to general readers. The 'www' language topics, on the other hand, serve a more specialized audience interested in the inner workings and technical infrastructure of the Wikimedia platforms.\n",
    "\n",
    "These pages will be removed from the dataset to streamline the model for predicting regular Wikipedia page traffic."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae949f875434b662"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "languages_to_exclude = ['www', 'commons']\n",
    "\n",
    "# Filter the dataset to exclude pages with the 'www' or 'commons' languages\n",
    "filtered_data = merged_data[~merged_data['Language'].isin(languages_to_exclude)]\n",
    "\n",
    "# Display the number of remaining pages and a sample of the filtered data\n",
    "print(f\"Total number of pages after excluding 'www' and 'commons': {len(filtered_data)}\")\n",
    "print(filtered_data.head())\n",
    "\n",
    "# Save the filtered dataset to a new CSV file if needed\n",
    "filtered_data.to_csv('Filtered_Wikipedia_Traffic.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d964aab9d4adbc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('Filtered_Wikipedia_Traffic.csv')\n",
    "df['Language'].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59d26a288960e02c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "merged_data = pd.read_csv('Filtered_Wikipedia_Traffic.csv')\n",
    "\n",
    "# Assuming the date columns start from the second column\n",
    "date_columns = merged_data.columns[1:]\n",
    "\n",
    "# Convert the date columns to datetime format\n",
    "merged_data.columns = ['Page'] + list(pd.to_datetime(date_columns, format='%Y-%m-%d', errors='coerce'))\n",
    "\n",
    "# Melt the dataframe to long format\n",
    "melted_data = pd.melt(merged_data, id_vars=['Page'], var_name='Date', value_name='Visits')\n",
    "\n",
    "# Drop rows with missing values in the 'Visits' column\n",
    "melted_data = melted_data.dropna(subset=['Visits'])\n",
    "\n",
    "# Ensure that 'Date' column is in datetime format\n",
    "melted_data['Date'] = pd.to_datetime(melted_data['Date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "# Extract year and month from the 'Date' column\n",
    "melted_data['Year'] = melted_data['Date'].dt.year\n",
    "melted_data['Month'] = melted_data['Date'].dt.month\n",
    "\n",
    "# Filter the data for the years 2015 to 2017\n",
    "filtered_data = melted_data[(melted_data['Year'] >= 2015) & (melted_data['Year'] <= 2017)]\n",
    "\n",
    "# Group by 'Year' and 'Month' to get the average visits per month\n",
    "monthly_avg_visits = filtered_data.groupby(['Year', 'Month'])['Visits'].mean().reset_index()\n",
    "\n",
    "# Pivot the data to have months as rows and years as columns\n",
    "pivoted_data = monthly_avg_visits.pivot(index='Month', columns='Year', values='Visits')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "pivoted_data.plot(kind='bar', figsize=(12, 8))\n",
    "\n",
    "plt.title('Average Monthly Wikipedia Page Visits (2015-2017)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Average Visits', fontsize=12)\n",
    "\n",
    "# Customize the x-axis to show month names\n",
    "plt.xticks(np.arange(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "\n",
    "# Add grid lines for clarity\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7196d8ecbef9ad5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming the date columns start from the second column\n",
    "date_columns = merged_data.columns[1:]\n",
    "\n",
    "# Convert the date columns to datetime format\n",
    "merged_data.columns = ['Page'] + list(pd.to_datetime(date_columns, format='%Y-%m-%d', errors='coerce'))\n",
    "\n",
    "# Add Topic and Language columns\n",
    "merged_data['Topic'] = merged_data['Page'].apply(lambda x: '_'.join(x.split('_')[:-3]))  # Extract topic\n",
    "merged_data['Language'] = merged_data['Page'].apply(lambda x: x.split('_')[-3].split('.')[0])  # Extract language\n",
    "\n",
    "# Melt the dataframe to long format\n",
    "melted_data = pd.melt(merged_data, id_vars=['Page', 'Topic', 'Language'], var_name='Date', value_name='Visits')\n",
    "\n",
    "# Drop rows with missing values in the 'Visits' column\n",
    "melted_data = melted_data.dropna(subset=['Visits'])\n",
    "\n",
    "# Ensure that 'Visits' column is numeric\n",
    "melted_data['Visits'] = pd.to_numeric(melted_data['Visits'], errors='coerce')\n",
    "\n",
    "# Group by Language and Topic, summing the visits\n",
    "grouped_data = melted_data.groupby(['Language', 'Topic'])['Visits'].sum().reset_index()\n",
    "\n",
    "# For each language, find the topic with the most visits\n",
    "most_popular_topic_per_language = grouped_data.loc[grouped_data.groupby('Language')['Visits'].idxmax()]\n",
    "\n",
    "# Display the most popular topics per language\n",
    "print(most_popular_topic_per_language)\n",
    "\n",
    "# Optional: Sort and display the top 5 most visited topics overall\n",
    "top_5_topics_overall = grouped_data.sort_values(by='Visits', ascending=False).head(5)\n",
    "print(\"\\nTop 5 most visited topics overall:\")\n",
    "print(top_5_topics_overall)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7975287a28ae30d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a4b62ca7e62b1cb0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
